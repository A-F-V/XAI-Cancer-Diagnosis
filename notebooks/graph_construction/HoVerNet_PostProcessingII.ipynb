{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HoVerNet Post Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "parent = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "if parent not in sys.path:\n",
    "    sys.path.append(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output of Model\n",
    "Here, I visualize what the output of the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "from src.model.architectures.graph_construction.hover_net import HoVerNet\n",
    "\n",
    "checkpoint_path = os.path.join(parent,\"model\",\"HoVerNet_Pan_Pre.ckpt\")\n",
    "args = {\"RESNET_SIZE\":50, \"START_LR\":0,}\n",
    "model = HoVerNet.load_from_checkpoint(checkpoint_path,categories=True,**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMS\n",
    "from torchvision.transforms import Compose,RandomApply\n",
    "from src.transforms.image_processing.augmentation import *\n",
    "\n",
    "\n",
    "scale_modes = {\"image\": InterpolationMode.BILINEAR, \"semantic_mask\": InterpolationMode.NEAREST, \"instance_mask\": InterpolationMode.NEAREST,\"categorical_mask\":InterpolationMode.NEAREST}\n",
    "transforms = Compose([\n",
    "\n",
    "\n",
    "        RandomCrop(size=(128, 128)),\n",
    "\n",
    "        ]) \n",
    "\n",
    "norm =         Normalize(\n",
    "                {\"image\": [0.6441, 0.4474, 0.6039]},\n",
    "                {\"image\": [0.1892, 0.1922, 0.1535]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "\n",
    "from src.datasets.PanNuke import PanNuke\n",
    "from src.datasets.MoNuSeg import MoNuSeg\n",
    "\n",
    "folder=  os.path.join(parent,\"data\",\"processed\",\"MoNuSeg_TRAIN\")\n",
    "dataset_val = MoNuSeg(folder,transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE\n",
    "from tqdm import tqdm\n",
    "sample = [dataset_val[i] for i in tqdm(range(5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utilities.img_utilities import tensor_to_numpy,numpy_to_tensor\n",
    "from src.vizualizations.image_viz import plot_images\n",
    "images = [tensor_to_numpy(img['image']) for img in sample]\n",
    "plot_images(images,(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTIONS\n",
    "\n",
    "model.eval()\n",
    "model.cpu()\n",
    "predictions = []\n",
    "for item in tqdm(sample,total = len(sample)):\n",
    "    sm,hv,c = model(norm(item)[\"image\"].unsqueeze(0))\n",
    "    sm = sm.squeeze().detach().cpu()\n",
    "    hv = hv.squeeze().detach().cpu()\n",
    "    predictions.append({\"semantic_mask\":sm,\"hover_map\":hv,\"semantic_mask_hard\":(sm>0.5).int(),\"category\":c})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0][\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY\n",
    "\n",
    "\n",
    "h_maps = [img[\"hover_map\"][0] for img in sample] + [img[\"hover_map\"][0] for img in predictions]\n",
    "plot_images(list(map(np.asarray,h_maps)),(2,5),\"jet\", vmin=-1,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_maps = [img[\"hover_map\"][1] for img in sample] + [img[\"hover_map\"][1] for img in predictions]\n",
    "plot_images(list(map(np.asarray,v_maps)),(2,5),\"jet\", vmin=-1,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = [img[\"semantic_mask\"].squeeze() for img in sample] +[img[\"semantic_mask\"] for img in predictions]\n",
    "plot_images(sms,(2,5),\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters\n",
    "Here, I explore some image processing filters, such as the Sobel Filter for edge detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFilter\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.nn.functional import conv2d\n",
    "from torch import Tensor\n",
    "import cv2\n",
    "\n",
    "sobel_x = Tensor([[-1,0,1],[-2,0,2],[-1,0,1]]).unsqueeze(0).unsqueeze(0).float()/4\n",
    "sobel_y = Tensor([[1,2,1],[0,0,0],[-1,-2,-1]]).unsqueeze(0).unsqueeze(0).float()/4\n",
    "\n",
    "def sobel(img:Tensor):\n",
    "    assert len(img.shape)==2\n",
    "    \n",
    "    img_np = img.numpy()\n",
    "    img_normed = cv2.normalize(img_np,None,alpha=0,beta=1,norm_type=cv2.NORM_MINMAX,dtype=cv2.CV_32F)\n",
    "    sobel_x = cv2.Sobel(img_normed,cv2.CV_32F,1,0,ksize=11)\n",
    "    sobel_y = cv2.Sobel(img_normed,cv2.CV_32F,0,1,ksize=11)\n",
    "    sx_normed = cv2.normalize(sobel_x,None,alpha=0,beta=1,norm_type=cv2.NORM_MINMAX,dtype=cv2.CV_32F)\n",
    "    sy_normed = cv2.normalize(sobel_y,None,alpha=0,beta=1,norm_type=cv2.NORM_MINMAX,dtype=cv2.CV_32F)\n",
    "    return torch.as_tensor(sx_normed),torch.as_tensor(sy_normed)\n",
    "    #img = img.unsqueeze(0).unsqueeze(0)\n",
    "    #img_x = conv2d(img,sobel_x,stride=1,padding=1)\n",
    "    #img_y = conv2d(img,sobel_y,stride=1,padding=1)\n",
    "    #return img_x,img_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_maps_sobel = [sobel(img.clip(-1,1)) for img in v_maps]\n",
    "plot_images([np.asarray(img[1]) for img in v_maps_sobel],(2,5),\"jet\",vmin=0,vmax=1)\n",
    "\n",
    "# you can see cells are 0 in first and non zero in second. This because horizontal grad perp to vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _S(hv_map:Tensor):\n",
    "    hv_horiz,hv_vert = hv_map\n",
    "    hpx = sobel(hv_horiz.float())[0].abs()\n",
    "    hpy = sobel(hv_vert.float())[1].abs()\n",
    "    return torch.maximum(1-hpx,1-hpy).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the sobel operator on the hover maps, we can start to see outlines of overlapping cells. On the top are the ground truth hover maps and on the bottom are predicted hovermaps. There are some intersting artefacts on the cells, making them look like loaves of bread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = [_S(img) for img in zip(h_maps,v_maps)]\n",
    "plot_images(list(map(np.asarray,importance)),(2,5),\"jet\",vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "h = 0.5\n",
    "k = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from skimage.morphology import remove_small_objects\n",
    "from scipy.ndimage import binary_fill_holes,binary_closing\n",
    "from scipy.ndimage.measurements import label\n",
    "from skimage.segmentation import watershed\n",
    "from src.utilities.tensor_utilties import reset_ids\n",
    "\n",
    "def post_processing_pipeline(sm:Tensor,hv_map:Tensor,h=0.5,k=0.5,smooth_amt = 7): #todo doc and annotate\n",
    "    Sm = _S(hv_map)\n",
    "    thresh_q = (sm > h)\n",
    "    thresh_q = torch.as_tensor(remove_small_objects(thresh_q.numpy(), min_size=30))\n",
    "    Sm = (Sm - (1-thresh_q.float())).clip(0)  # importance regions with background haze removed via mask with clipping\n",
    "\n",
    "    # to get areas of low importance (i.e. centre of cells) as high energy and areas close to border are low energy\n",
    "    energy = (1-Sm)*thresh_q\n",
    "    # also clip again background\n",
    "    energy = torch.as_tensor(cv2.GaussianBlur(energy.numpy(), (smooth_amt, smooth_amt), 0)\n",
    "                             )  # smooth landscape # especially important for long cells\n",
    "\n",
    "    markers = (thresh_q.float() - (Sm > k).float())\n",
    "    markers = label(markers)[0]\n",
    "    # Slightly different to paper - I use the energy levels instead because they have been smoothed\n",
    "    markersv2 = (energy > k).numpy()\n",
    "    markersv2 = binary_fill_holes(markersv2)\n",
    "    markersv2 = label(markersv2)[0]\n",
    "    return torch.as_tensor(watershed(-energy.numpy(), markers=markersv2, mask=thresh_q.numpy()), dtype=torch.int)\n",
    "\n",
    "\n",
    "ins_gt = [reset_ids(img[\"instance_mask\"].squeeze().numpy()) for img in sample]\n",
    "ins_pred = [post_processing_pipeline(sm,hv_map,k=0.5,smooth_amt=5) for sm,hv_map in zip(sms,zip(h_maps,v_maps))]\n",
    "plot_images(ins_gt+ins_pred,(3,5),\"jet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, what is important is that the GT Semantic Mask and Hover Maps produce perfect segmentation, which they do above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Type Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[0].keys())\n",
    "cat_examples = [img['category'].argmax(dim=1).squeeze() for img in predictions]+[img['category_mask'].argmax(dim=0) for img in sample]\n",
    "print(cat_examples[0].shape)\n",
    "plot_images(cat_examples,(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transforms.graph_construction.hovernet_post_processing import assign_instance_class_label\n",
    "\n",
    "ins_1 =  ins_pred[0+5]\n",
    "ins_gt_1 = torch.as_tensor(ins_gt[0])\n",
    "nc_1 = predictions[0]['category'].squeeze()\n",
    "#nc_gt_1 = sample[0]['category_mask'].squeeze()\n",
    "img_1 = images[0].squeeze()\n",
    "\n",
    "cell_pred_1 = assign_instance_class_label(ins_1,nc_1)\n",
    "#cell_gt_1 = assign_instance_class_label(ins_gt_1,nc_gt_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.ma import masked_where\n",
    "from src.transforms.graph_construction.percolation import hollow\n",
    "import numpy as np\n",
    "\n",
    "def create_coloured_mask(mask:np.ndarray,colour):\n",
    "    if len(mask.shape) == 2:\n",
    "        mask = np.expand_dims(mask,axis=2).repeat(3,axis=2)  # introduce new axis and then fill with repeat\n",
    "    colour = np.asarray(colour)\n",
    "    coloured_mask = np.zeros_like(mask)+colour\n",
    "    return coloured_mask*mask\n",
    "\n",
    "def instance_segmentation_vizualised(img:np.ndarray, instance_seg:np.ndarray, cat_pred:np.ndarray, figsize=(20, 20)):\n",
    "    \"\"\"Plots image and the segmentation overlayed on top\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): Original Image (H,W,3)\n",
    "        instance_seg (np.ndarray): Instance Segmentation of Image (same size) (H,W)\n",
    "        cat_pred (np.ndarray): List of cell type predictions. cat_pred[i] is cell i's prediction\n",
    "    \"\"\"\n",
    "    assert img.shape[:2] == instance_seg.shape[:], \"Image and instance segmentation must be same size\"\n",
    "\n",
    "    colour_scheme = [[1.,0.,0.],[1.0,0.5,0.],[0.,1.,0.],[0.,0.,1.],[1.0,1.0,0.]] # neo - red, non-neo - orange, inflam - green, conn - blue, dead - yell\n",
    "    hl = tensor_to_numpy(hollow(instance_seg))\n",
    "    \n",
    "    \n",
    "    hollow_masks = [np.isin(hl,np.nonzero(cat_pred==cell_type)) for cell_type in range(0,5)]\n",
    "    masks = [create_coloured_mask(hm,colour) for i,colour,hm in zip(range(5),colour_scheme,hollow_masks)]\n",
    "    final_mask = np.sum(masks,axis=0)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(tensor_to_numpy(img))\n",
    "    plt.imshow(final_mask,alpha=0.7)\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(img_1.shape)\n",
    "instance_segmentation_vizualised(img_1,ins_1,cell_pred_1,figsize=(5,5))\n",
    "#instance_segmentation_vizualised(img_1,np.asarray(ins_gt_1),cell_gt_1,figsize=(5,5))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "281c2a3310c6ee84981005ada6d27171c5222c196ec4eb053b8cd9b83c5ea575"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
