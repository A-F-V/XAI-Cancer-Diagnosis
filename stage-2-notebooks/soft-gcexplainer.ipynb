{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft-GCExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\git\\XAI-Cancer-Diagnosis\\XAI-Cancer-Diagnosis\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "\n",
    "parent = os.path.dirname(os.getcwd())\n",
    "if parent not in sys.path:\n",
    "    sys.path.append(parent)\n",
    "print(parent)\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_settings = {'evaluate_model':False, 'save_concepts': False, 'tsne':False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available:  True\n",
      "CUDA device count:  1\n",
      "78\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "from src.datasets.BACH import BACH\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import Compose, KNNGraph\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "graph_aug_val = Compose([KNNGraph(k=6)])\n",
    "\n",
    "\n",
    "print('CUDA available: ', torch.cuda.is_available())\n",
    "print('CUDA device count: ', torch.cuda.device_count())\n",
    "\n",
    "src_folder = os.path.join(\n",
    "    \"C://Users\", \"aless\", \"Documents\", \"FtT\", \"data\", \"BACH_TRAIN\")\n",
    "tid,vid = BACH.get_train_val_ids(src_folder,\"graph_ind_FtT_19_11_dropout_3.txt\")\n",
    "train_set = BACH(src_folder,graph_augmentation=graph_aug_val,ids=tid,pre_encoded=True)\n",
    "val_set = BACH(src_folder,graph_augmentation=graph_aug_val,ids=vid,pre_encoded=True)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=4, shuffle=False)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CancerGNN(\n",
      "  (gnn): GCNx(\n",
      "    (conv): ModuleList(\n",
      "      (0): GCNConv(64, 64)\n",
      "      (1): GCNConv(64, 64)\n",
      "      (2): GCNConv(64, 64)\n",
      "      (3): GCNConv(64, 64)\n",
      "      (4): GCNConv(64, 64)\n",
      "      (5): GCNConv(64, 64)\n",
      "      (6): GCNConv(64, 64)\n",
      "    )\n",
      "    (transform): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): BatchNorm1d(312, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=312, out_features=64, bias=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0, inplace=False)\n",
      "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predictor): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0, inplace=False)\n",
      "    (5): Linear(in_features=64, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=False)\n",
      "  (1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (2): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (3): ReLU()\n",
      "  (4): Dropout(p=0, inplace=False)\n",
      "  (5): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "from src.deep_learning.architectures.cancer_prediction.cancer_gnn import CancerGNN\n",
    "\n",
    "model = CancerGNN.load_from_checkpoint(os.path.join(parent,\"experiments\",\"checkpoints\", \"FtT_FtT_19_11_dropout_3.ckpt\"),WIDTH=64,HEIGHT=7)\n",
    "\n",
    "print(model)\n",
    "print(model.predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch, Data\n",
    "\n",
    "def loader_from_one_graph(graph:Data):\n",
    "    return DataLoader([graph], batch_size=1, shuffle=False)\n",
    "\n",
    "def extract_graph(batch: Batch, graph_idx: int):\n",
    "    # Find node indices for the graph\n",
    "    node_mask = batch.batch == graph_idx\n",
    "\n",
    "    # Extract the node features for the graph\n",
    "    x = batch.x[node_mask]\n",
    "\n",
    "    # Find edge indices for the graph\n",
    "    edge_mask = node_mask[batch.edge_index[0]] & node_mask[batch.edge_index[1]]\n",
    "    edge_index = batch.edge_index[:, edge_mask]\n",
    "\n",
    "    # Re-map edge indices to the new node index space\n",
    "    edge_index = edge_index - node_mask.nonzero(as_tuple=False).min()\n",
    "\n",
    "    # If the batch contains other attributes, extract them similarly\n",
    "    # ...\n",
    "    y = batch.y[graph_idx]\n",
    "    pos = batch.pos[node_mask]\n",
    "\n",
    "    # Create a new Data object for the single graph\n",
    "    single_graph = Data(x=x, edge_index=edge_index, y=y,pos=pos)\n",
    "    \n",
    "    return single_graph\n",
    "\n",
    "def batch_to_graphs(batch:Batch):\n",
    "    num_graphs = batch.batch.max().item()\n",
    "    for ind in range(num_graphs):\n",
    "        yield extract_graph(batch,ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Raw Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Width:  64\n"
     ]
    }
   ],
   "source": [
    "model_width = model.width\n",
    "print(\"Model Width: \",model_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch import softmax\n",
    "\n",
    "class RawActivationHook:\n",
    "    def __init__(self,width,device='cuda'):\n",
    "        self.activations = torch.zeros(0,width).to(device)\n",
    "    \n",
    "    def append_activations(self, model,input, output):\n",
    "        self.activations = torch.cat((self.activations,output),dim=0)\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.remove_handle = model.gnn.conv[-1].register_forward_hook(lambda m,i,o:(self.append_activations(m,i,o)))\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.remove_handle.remove()\n",
    "        \n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n",
    "class ModelForward:\n",
    "    def __init__(self,model,verbose=False):\n",
    "        self.model = model\n",
    "        self.model_width  = model.width\n",
    "        self.model.eval()\n",
    "        self.model.to('cuda')\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def forward_node_level(self,loader):\n",
    "        # Acquire the activations\n",
    "        hook = RawActivationHook(self.model_width)\n",
    "\n",
    "\n",
    "        \n",
    "        predictions = torch.zeros(0).to('cuda')\n",
    "        ground = torch.zeros(0).to('cuda')\n",
    "        \n",
    "        with hook:\n",
    "            for batch in tqdm(loader,disable=not self.verbose):\n",
    "                batch = batch.to('cuda')\n",
    "                output = model.forward(batch.x,batch.edge_index,batch.batch)\n",
    "                \n",
    "                node_probs = softmax(output,dim=1)[batch.batch]\n",
    "                node_ground = batch.y[batch.batch]\n",
    "                \n",
    "                \n",
    "                predictions = torch.cat([predictions,node_probs])\n",
    "                ground = torch.cat([ground,node_ground])\n",
    "            \n",
    "        raw_activations = hook.get_activations()\n",
    "        return raw_activations.cpu().detach().numpy(),predictions.cpu().detach().numpy(),ground.cpu().detach().numpy().astype(int)\n",
    "    \n",
    "    def forward_graph_level(self,loader):\n",
    "        predictions = torch.zeros(0).to('cuda')\n",
    "        ground = torch.zeros(0).to('cuda')\n",
    "        \n",
    "        for batch in tqdm(loader,disable=not self.verbose):\n",
    "            batch = batch.to('cuda')\n",
    "            output = model.forward(batch.x,batch.edge_index,batch.batch)\n",
    "            \n",
    "            graph_probs = softmax(output,dim=1)\n",
    "            graph_ground = batch.y\n",
    "            \n",
    "            predictions = torch.cat([predictions,graph_probs])\n",
    "            ground = torch.cat([ground,graph_ground])\n",
    "            \n",
    "        return predictions.cpu().detach().numpy(),ground.cpu().detach().numpy().astype(int)\n",
    "\n",
    "forwarder = ModelForward(model)\n",
    "\n",
    "graph_prob_predictions_train,graph_ground_train = forwarder.forward_graph_level(train_loader)\n",
    "graph_prob_predictions_val,graph_ground_val = forwarder.forward_graph_level(val_loader)\n",
    "\n",
    "activations_train,node_predictions_train,node_ground_train  = forwarder.forward_node_level(train_loader)\n",
    "activations_val,node_predictions_val,node_ground_val = forwarder.forward_node_level(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_predictions_train = np.argmax(graph_prob_predictions_train,axis=1)\n",
    "graph_predictions_val = np.argmax(graph_prob_predictions_val,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on validation set: 0.7236842105263158\n",
      "Model accuracy on training set: 0.8673139158576052\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model accuracy on validation set: {np.sum(graph_predictions_val == graph_ground_val)/len(graph_ground_val)}\")\n",
    "print(f\"Model accuracy on training set: {np.sum(graph_predictions_train == graph_ground_train)/len(graph_ground_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture,BayesianGaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "class ConceptDiscoverer:\n",
    "    def __init__(self,num_concepts,verbose=True,whiten=True,**kwargs) -> None:\n",
    "        self.k = num_concepts\n",
    "        self.gm = MiniBatchKMeans(n_clusters=self.k,verbose=verbose,**kwargs)\n",
    "        self.whiten_act = whiten\n",
    "        \n",
    "    def fit(self,activations: np.ndarray):\n",
    "        self.mu = activations.mean(axis=0)\n",
    "        self.sigma = activations.std(axis=0)\n",
    "        \n",
    "        whitten_activations = self.whiten(activations)\n",
    "        \n",
    "        self.gm.fit(whitten_activations)\n",
    "    \n",
    "    def predict(self,activations):\n",
    "        whitten_activations = self.whiten(activations)\n",
    "        return self.gm.predict(whitten_activations)\n",
    "        \n",
    "    def get_concept_distances(self,activations):\n",
    "        whitten_activations = self.whiten(activations)\n",
    "        # Get the distances from the cluster centers\n",
    "        return self.gm.transform(whitten_activations)\n",
    "    \n",
    "    def silouhette_score(self,activations,**kwargs):\n",
    "        whitten_activations = self.whiten(activations)\n",
    "        return silhouette_score(whitten_activations,self.gm.predict(whitten_activations),**kwargs)\n",
    "        \n",
    "    def whiten(self,obs):\n",
    "        if(self.whiten_act):\n",
    "            return (obs - self.mu)/self.sigma\n",
    "        else:\n",
    "            return obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method: k-means++\n",
      "Inertia for init 1/3: 3030.372559\n",
      "Init 2/3 with method: k-means++\n",
      "Inertia for init 2/3: 3582.701660\n",
      "Init 3/3 with method: k-means++\n",
      "Inertia for init 3/3: 2914.075195\n",
      "Minibatch iteration 1/283100: mean batch inertia: 13.445592, ewa inertia: 13.445592 \n",
      "Minibatch iteration 2/283100: mean batch inertia: 16.711725, ewa inertia: 13.447900 \n",
      "Minibatch iteration 3/283100: mean batch inertia: 18.364779, ewa inertia: 13.451374 \n",
      "Minibatch iteration 4/283100: mean batch inertia: 14.972295, ewa inertia: 13.452449 \n",
      "Minibatch iteration 5/283100: mean batch inertia: 15.425530, ewa inertia: 13.453843 \n",
      "Minibatch iteration 6/283100: mean batch inertia: 15.789164, ewa inertia: 13.455493 \n",
      "Minibatch iteration 7/283100: mean batch inertia: 15.005930, ewa inertia: 13.456589 \n",
      "Minibatch iteration 8/283100: mean batch inertia: 15.489277, ewa inertia: 13.458025 \n",
      "Minibatch iteration 9/283100: mean batch inertia: 18.087726, ewa inertia: 13.461296 \n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "Minibatch iteration 10/283100: mean batch inertia: 14.564994, ewa inertia: 13.462076 \n",
      "Minibatch iteration 11/283100: mean batch inertia: 16.449415, ewa inertia: 13.464187 \n",
      "Converged (lack of improvement in inertia) at iteration 11/283100\n",
      "Computing label assignment and total inertia\n"
     ]
    }
   ],
   "source": [
    "sub_sample_size = 10000\n",
    "sub_sample = np.random.choice(activations_train.shape[0],sub_sample_size,replace=False)\n",
    "\n",
    "num_concepts =64\n",
    "\n",
    "cd = ConceptDiscoverer(num_concepts=num_concepts)\n",
    "cd.fit(activations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing label assignment and total inertia\n",
      "Computing label assignment and total inertia\n"
     ]
    }
   ],
   "source": [
    "train_concept_probs = cd.predict_proba(activations_train)\n",
    "val_concept_probs = cd.predict_proba(activations_val)\n",
    "\n",
    "train_concepts = cd.predict(activations_train)\n",
    "val_concepts = cd.predict(activations_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection Sampling\n",
    "What if we consider only those activations that are we have a certain degree of confidence is in that concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_acceptance(probabilities:np.ndarray,threshold = 0.99):\n",
    "    max_prob = probabilities.max(axis=1)\n",
    "    assert(max_prob.shape[0] == probabilities.shape[0])\n",
    "    number_surpass = (max_prob > threshold).sum()\n",
    "    return number_surpass / probabilities.shape[0]\n",
    "\n",
    "#print(\"Train: \",within_acceptance(train_concept_probs))\n",
    "#print(\"Val: \",within_acceptance(val_concept_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook_settings['save_concepts']:\n",
    "    np.save(\"mu.npy\",mu)\n",
    "    np.save(\"sigma.npy\",sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Vizualize Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "if notebook_settings['tsne']:\n",
    "    tsne_sample_size  = 50000\n",
    "    tsne_sample= np.random.choice(activations_train.shape[0],tsne_sample_size,replace=False)\n",
    "    position_tsne_sample =TSNE(n_components=2,verbose=2,perplexity = 2000,n_iter=500).fit_transform(activations_train[tsne_sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def plot_tsne_points_with_category(points,classes):\n",
    "    f = plt.figure(figsize=(10,10))\n",
    "    # How many different classes are there\n",
    "    unique_classes = np.unique(classes)\n",
    "    colours = cm.rainbow(np.linspace(0,1,len(unique_classes)))\n",
    "    \n",
    "    # Group the points by their class and then plot all points of the same class at a time\n",
    "    for i,cl in enumerate(unique_classes):\n",
    "        points_with_class = (classes==cl)\n",
    "        v = points[points_with_class]\n",
    "        x,y = v[:,0],v[:,1]\n",
    "        plt.scatter(x,y,color=colours[i])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook_settings['tsne']:\n",
    "    prediction_tsne_sample = node_predictions_train[tsne_sample].argmax(axis=1)\n",
    "    ground_tsne_sample = node_ground_train[tsne_sample]\n",
    "    \n",
    "    k = 64\n",
    "    cd = ConceptDiscoverer(num_concepts=k,verbose=False)\n",
    "    cd.fit(activations_train)\n",
    "    hard_concept_tsne_sample = cd.predict(activations_train[tsne_sample])\n",
    "\n",
    "    print(prediction_tsne_sample.shape)\n",
    "    plot_tsne_points_with_category(position_tsne_sample,ground_tsne_sample)\n",
    "    plot_tsne_points_with_category(position_tsne_sample,hard_concept_tsne_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Concept Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_concept(concept_ids,num_concepts):\n",
    "    one_hot = np.zeros((len(concept_ids),num_concepts))\n",
    "    one_hot[np.arange(len(concept_ids)),concept_ids] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Takes every observed concept along with the eventual graph prediction and then generates X,Y pairs for training a classifier\n",
    "def generate_concept_to_prediction_map(model,cd,loader):\n",
    "    f = ModelForward(model)\n",
    "    act,pred_prob,ground =  f.forward_node_level(loader)\n",
    "    \n",
    "    return cd.predict_proba(act),ground\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ConceptNodeCancerPredictor:\n",
    "    def __init__(self,k):\n",
    "        self.dt = DecisionTreeClassifier(max_leaf_nodes=k)\n",
    "        self.k = k\n",
    "        \n",
    "    \n",
    "    def fit(self,concepts,graph_ground):\n",
    "        self.dt.fit(concepts.reshape(-1,1),graph_ground)\n",
    "\n",
    "    \n",
    "    def predict(self,concepts):\n",
    "        return self.predict_proba(concepts.reshape(-1,1)).argmax(axis=1)\n",
    "    \n",
    "    def predict_proba(self,concepts):\n",
    "        return self.dt.predict_proba(concepts)\n",
    "        \n",
    "    def get_concept_prob_of_cancer(self):\n",
    "        return self.dt.predict_proba(np.arange(self.k).reshape(-1,1))\n",
    "\n",
    "class ConceptGraphClassPredictor:\n",
    "    def __init__(self,k,trained_model,trained_concept_discoverer, trained_node_graph_class_predictor):\n",
    "        self.node_predictor = ConceptNodeCancerPredictor(k)\n",
    "        self.k = k\n",
    "        self.forwarder = ModelForward(trained_model)\n",
    "        self.cd = trained_concept_discoverer\n",
    "        self.node_predictor = trained_node_graph_class_predictor\n",
    "        \n",
    "    def get_graph_concept_distribution(self, graph, rejection_threshold = 0.99):\n",
    "        act,pred_prob,ground = self.forwarder.forward_node_level(loader_from_one_graph(graph))\n",
    "        graph_pred = pred_prob[0]\n",
    "        graph_ground = ground[0]\n",
    "        #ensure graph ground is flattened\n",
    "        ground = ground.reshape(-1)\n",
    "\n",
    "        concept_prob = one_hot_concept(self.cd.predict(act),self.k)\n",
    "        num_nodes = graph.x.shape[0]\n",
    "        \n",
    "        assert concept_prob.shape == (num_nodes,self.k)\n",
    "        \n",
    "                \n",
    "        # Filter out concepts that are not accepted\n",
    "        accepted = (concept_prob.max(axis=1) > rejection_threshold)\n",
    "        num_remaining_nodes =  accepted.sum()\n",
    "        \n",
    "        assert accepted.shape == (num_nodes,)\n",
    "        concept_prob = concept_prob[accepted]\n",
    "        assert concept_prob.shape == (num_remaining_nodes,self.k)\n",
    "        pred_prob = pred_prob[accepted]\n",
    "        assert pred_prob.shape == (num_remaining_nodes,4)\n",
    "        ground = ground[accepted]\n",
    "        assert ground.shape == (num_remaining_nodes,)\n",
    "        \n",
    "        concept_dist = concept_prob.mean(axis=0)\n",
    "        assert concept_dist.shape == (self.k,)\n",
    "        \n",
    "        \n",
    "        # Graph-pred/ground is the prediction and ground truth for the graph (one value)\n",
    "        # Node-pred/ground is the prediction and ground truth for the graph but one for each node\n",
    "        return {'concept-dist':concept_dist,'node-pred':pred_prob, 'node-ground':ground, 'graph-pred':graph_pred, 'graph-ground':graph_ground, 'num-nodes':num_nodes, 'num-nodes-above-threshold':num_remaining_nodes}\n",
    "    \n",
    "    def vote_on_graph(self,graph,rejection_threshold = 0.99):\n",
    "        prediction_info = self.get_graph_concept_distribution(graph,rejection_threshold)\n",
    "        concept_prob_of_class = self.node_predictor.get_concept_prob_of_cancer()\n",
    "        # Perform a weighted sum vote\n",
    "        assert prediction_info['concept-dist'].shape == (self.k,)\n",
    "        \n",
    "        weighted_sum = prediction_info['concept-dist'].reshape(-1,1)*concept_prob_of_class\n",
    "        \n",
    "        assert concept_prob_of_class.shape == (self.k,4)\n",
    "        \n",
    "        assert weighted_sum.shape == (self.k,4)\n",
    "        vote = weighted_sum.sum(axis=0)\n",
    "                \n",
    "        assert vote.shape == (4,)\n",
    "        prediction_info['concept-vote'] = vote\n",
    "        return prediction_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: For a particular graph, what concepts are present, how many times for each, and what is the contribution of that concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cncp = ConceptNodeCancerPredictor(num_concepts)\n",
    "cncp.fit(train_concepts,node_ground_train)\n",
    "cgcp = ConceptGraphClassPredictor(num_concepts,model,cd,cncp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing label assignment and total inertia\n",
      "Ground:  0\n",
      "Prediction:  0 [0.8672594  0.06075229 0.01158582 0.0604026 ]\n",
      "Concept vote:  0 [0.40886307 0.27866339 0.11668984 0.1957837 ]\n"
     ]
    }
   ],
   "source": [
    "random_graph_id = np.random.randint(len(val_loader))\n",
    "graph = train_loader.dataset[random_graph_id]\n",
    "\n",
    "prediction_info = cgcp.vote_on_graph(graph)\n",
    "\n",
    "graph_concept_vote = prediction_info['concept-vote']\n",
    "\n",
    "graph_ground = prediction_info['graph-ground']\n",
    "graph_prediction = prediction_info['graph-pred']\n",
    "\n",
    "print(\"Ground: \",graph_ground)\n",
    "print(\"Prediction: \",graph_prediction.argmax().item(),graph_prediction)\n",
    "print(\"Concept vote: \",graph_concept_vote.argmax(),graph_concept_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import cross entropy and accuracy\n",
    "from sklearn.metrics import accuracy_score,log_loss\n",
    "\n",
    "# Across a whole dataset, let us evaluate how good the concept_voter is\n",
    "\n",
    "\n",
    "class ConceptCompleteness:\n",
    "    def __init__(self, trained_model,num_concepts,verbose = False,rejection_threshold=0.99,**kwargs):\n",
    "        self.model = trained_model\n",
    "        self.k = num_concepts\n",
    "        self.max_samples_for_concept_discovery = kwargs.get('max_samples_for_concept_discovery',10000)\n",
    "        self.forwarder = ModelForward(self.model,verbose=verbose)\n",
    "        self.verbose = verbose\n",
    "        self.rejection_threshold = rejection_threshold\n",
    "        self.max_iter = kwargs.get('max_iter',200)\n",
    "        \n",
    "    def fit(self,loader):\n",
    "        activations,node_pred,node_ground = self.forwarder.forward_node_level(loader)\n",
    "        \n",
    "        self.cd = ConceptDiscoverer(self.k,verbose=self.verbose,max_iter=self.max_iter)\n",
    "        subset_of_activations = activations[np.random.choice(activations.shape[0],self.max_samples_for_concept_discovery,replace=False)]\n",
    "        self.cd.fit(subset_of_activations)\n",
    "        concepts = self.cd.predict(activations)\n",
    "        \n",
    "        self.cncp = ConceptNodeCancerPredictor(self.k)\n",
    "        self.cncp.fit(concepts,node_ground)\n",
    "            \n",
    "        self.cgcp = ConceptGraphClassPredictor(self.k,self.model,self.cd,self.cncp)\n",
    "        \n",
    "    def evaluate(self,graph):\n",
    "        prediction_info = self.cgcp.vote_on_graph(graph,self.rejection_threshold)\n",
    "        return prediction_info['concept-vote'].argmax(),prediction_info['graph-ground']\n",
    "            \n",
    "        \n",
    "    def completeness(self,loader):\n",
    "        Y_HAT = []\n",
    "        Y = []\n",
    "        for batch in tqdm(loader,disable=not self.verbose):\n",
    "            for graph_id in range(batch.batch.max()):\n",
    "                graph = extract_graph(batch,graph_id)\n",
    "                vote,ground = self.evaluate(graph)\n",
    "                Y_HAT.append(vote)\n",
    "                Y.append(ground)\n",
    "        matches = np.equal(np.array(Y),np.array(Y_HAT)).sum()\n",
    "        return matches/len(Y)\n",
    "    \n",
    "                \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7543859649122807"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeness = ConceptCompleteness(model,num_concepts,max_samples_for_concept_discovery=10000)\n",
    "completeness.fit(train_loader)\n",
    "completeness.completeness(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine K\n",
    "- Perform GM\n",
    "- Get Concept Completeness\n",
    "- Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 5, , Score: 0.631578947368421\n",
      "K: 10, , Score: 0.6666666666666666\n",
      "K: 15, , Score: 0.7543859649122807\n",
      "K: 20, , Score: 0.7017543859649122\n",
      "K: 25, , Score: 0.7192982456140351\n",
      "K: 30, , Score: 0.7017543859649122\n",
      "K: 35, , Score: 0.7368421052631579\n",
      "K: 40, , Score: 0.7017543859649122\n",
      "K: 45, , Score: 0.7192982456140351\n",
      "K: 50, , Score: 0.7368421052631579\n",
      "K: 55, , Score: 0.7192982456140351\n",
      "K: 60, , Score: 0.7017543859649122\n",
      "K: 65, , Score: 0.7192982456140351\n",
      "K: 70, , Score: 0.7192982456140351\n",
      "K: 75, , Score: 0.7017543859649122\n",
      "K: 80, , Score: 0.7719298245614035\n",
      "K: 85, , Score: 0.7543859649122807\n",
      "K: 90, , Score: 0.7192982456140351\n",
      "K: 95, , Score: 0.7192982456140351\n"
     ]
    }
   ],
   "source": [
    "k_to_inspect = np.arange(5,100,5)\n",
    "completeness_scores = []\n",
    "\n",
    "for k in k_to_inspect:\n",
    "    completeness = ConceptCompleteness(model,k,max_samples_for_concept_discovery=100000)\n",
    "    completeness.fit(train_loader)\n",
    "    score = completeness.completeness(val_loader)\n",
    "    completeness_scores.append((k,score))   \n",
    "    print(f\"K: {k}, , Score: {score}\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completeness_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33336/1899847711.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompleteness_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_to_inspect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompleteness_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'completeness_scores' is not defined"
     ]
    }
   ],
   "source": [
    "print(completeness_scores)\n",
    "plt.plot(k_to_inspect,completeness_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Silhoutte Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 2, Score: 0.17153768241405487\n",
      "K: 4, Score: 0.21925793588161469\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18876/443431010.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConceptDiscoverer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msiho_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilouhette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msiho_avg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"K: {k}, Score: {siho_avg}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18876/672999211.py\u001b[0m in \u001b[0;36msilouhette_score\u001b[1;34m(self, activations, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msilouhette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwhitten_activations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhitten_activations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhitten_activations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwhiten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    232\u001b[0m     reduce_func = functools.partial(_silhouette_reduce,\n\u001b[0;32m    233\u001b[0m                                     labels=labels, label_freqs=label_freqs)\n\u001b[1;32m--> 234\u001b[1;33m     results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func,\n\u001b[0m\u001b[0;32m    235\u001b[0m                                               **kwds))\n\u001b[0;32m    236\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1621\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1622\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1623\u001b[1;33m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0m\u001b[0;32m   1624\u001b[0m                                      n_jobs=n_jobs, **kwds)\n\u001b[0;32m   1625\u001b[0m         if ((X is Y or Y is None)\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   1788\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1790\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# To minimize precision issues with float32, we compute the distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;31m# matrix on chunks of X and Y upcast to float64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_euclidean_distances_upcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_euclidean_distances_upcast\u001b[1;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m                 \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_chunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m                 \u001b[0md\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX_chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m                 \u001b[0md\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY_chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k_to_inspect = [2,4,8,16,32,64,80,100]\n",
    "\n",
    "scores = []\n",
    "for k in k_to_inspect:\n",
    "    cd = ConceptDiscoverer(k,verbose=False,whiten=True,max_iter=2000)\n",
    "    cd.fit(activations_train)\n",
    "    siho_avg = cd.silouhette_score(activations_train,sample_size=50000)\n",
    "    scores.append((k,siho_avg))\n",
    "    print(f\"K: {k}, Score: {siho_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptGraphGenerator:\n",
    "    def __init__(self,model_forwarder,concept_discoverer):\n",
    "        self.forwarder = model_forwarder\n",
    "        self.cd = concept_discoverer\n",
    "        \n",
    "    def generate_concept_graph(self,graph):\n",
    "        # 1) Get the activations for the graph\n",
    "        act,pred_prob,ground = self.forwarder.forward_node_level(loader_from_one_graph(graph))\n",
    "        # 2) Get the concepts\n",
    "        concepts = self.cd.predict(act)\n",
    "        graph.concepts = concepts\n",
    "        graph.activations = act\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_concepts =32\n",
    "\n",
    "cd = ConceptDiscoverer(num_concepts=num_concepts,verbose=False)\n",
    "cd.fit(activations_train)\n",
    "cgg = ConceptGraphGenerator(ModelForward(model),cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopN:\n",
    "    def __init__(self, n, key=lambda x:x):\n",
    "        self.n = n\n",
    "        self.container = []\n",
    "        self.key = key\n",
    "    def add(self, element):\n",
    "        self.container.append(element)\n",
    "        self.container.sort(reverse=True, key=self.key)\n",
    "        if len(self.container) > self.n:\n",
    "            self.container = self.container[:self.n]\n",
    "    \n",
    "    def get_top(self):\n",
    "        return self.container\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class ConceptRepresentationExtractor:\n",
    "    def __init__(self,concept_graph_generator, concept_discoverer):\n",
    "        self.cgg = concept_graph_generator\n",
    "        self.cd = concept_discoverer\n",
    "        self.k = self.cd.k\n",
    "        \n",
    "    def generate_top_n_representations(self,loader,n,num_hops):\n",
    "        # Hold a list of the top n representations for each of the k concepts\n",
    "        top_n_representations = [TopN(n,lambda x:-x[0]) for i in range(self.k)]\n",
    "        for batch in loader:\n",
    "            for graph in batch_to_graphs(batch):\n",
    "                self.cgg.generate_concept_graph(graph)\n",
    "                for node_id in range(graph.x.shape[0]):\n",
    "                    subgraph = self.extract_representation(graph,node_id,num_hops)\n",
    "                    concept = graph.concepts[node_id]\n",
    "                    act = graph.activations[node_id]\n",
    "                    # Get the distance to the concept\n",
    "                    concept_dist = self.cd.get_concept_distances(act)[0][concept]\n",
    "                    \n",
    "                    top_n_representations[concept].add((concept_dist,subgraph))\n",
    "\n",
    "    def extract_representation(self,graph,node_id,n):\n",
    "        # Create a subgraph around the node\n",
    "        subset,edges,_,_, = k_hop_subgraph(node_id,n,graph.edge_index)\n",
    "        #\n",
    "        subgraph = Data(x=graph.x[subset],edge_index=edges)\n",
    "        subgraph.concepts = graph.concepts[subset]\n",
    "        subgraph.image = graph.image\n",
    "        return subgraph\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_extractor = ConceptRepresentationExtractor(cgg,cd)\n",
    "rep_extractor.generate_top_n_representations(train_loader,10,2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e591b006129f0dfb1d51895b4ed619a4625402ac1a377124080c35d47fc6e9a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
